{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1693ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48043f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Langchin helps build LLM applications\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrivers.\")\n",
    "]\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "dense_vectorstore = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriever = dense_vectorstore.as_retriever()\n",
    "\n",
    "# sparse Retriever (BM25)\n",
    "sparse_retriever = BM25Retriever.from_documents(docs)\n",
    "sparse_retriever.k = 3\n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[dense_retriever, sparse_retriever], \n",
    "                                     weights=[0.7, 0.3]\n",
    "                                     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13ceceaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000146D1FC7790>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x00000146D2697A50>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7298415b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document1:\n",
      "Langchin helps build LLM applications\n",
      "\n",
      "Document2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Document3:\n",
      "Langchain has many types of retrivers.\n",
      "\n",
      "Document4:\n",
      "Pinecone is a vector database for semantic search\n"
     ]
    }
   ],
   "source": [
    "query = \"How can I build applications with LLMs?\"\n",
    "result = hybrid_retriever.invoke(query)\n",
    "\n",
    "for i, doc in enumerate(result):\n",
    "    print(f\"\\nDocument{i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c524d48",
   "metadata": {},
   "source": [
    "### RAG Pipeline with Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a658303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q. How can I build an app using LLMs\n",
      "A. To build an app using Large Language Models (LLMs), you can follow these general steps:\n",
      "\n",
      "1. **Define the Purpose**: Clearly outline what you want your app to achieve. Determine the specific tasks or problems the LLM will address.\n",
      "\n",
      "2. **Choose the Right Tools**: Utilize frameworks like Langchain, which is designed to help build applications with LLMs. Langchain can assist in developing agentic AI applications and offers various retrievers to enhance functionality.\n",
      "\n",
      "3. **Select a Vector Database**: For efficient semantic search and retrieval, consider using a vector database like Pinecone. This will help in managing and querying large datasets effectively.\n",
      "\n",
      "4. **Data Preparation**: Gather and preprocess the data your application will use. This may involve cleaning, formatting, and structuring the data to ensure it is suitable for the LLM.\n",
      "\n",
      "5. **Model Selection**: Choose an appropriate LLM that fits your application's needs. This could be an existing model or a custom-trained one, depending on your requirements.\n",
      "\n",
      "6. **Integration**: Integrate the LLM with your application using APIs or SDKs provided by the model's platform. Langchain can facilitate this integration process.\n",
      "\n",
      "7. **Testing and Iteration**: Test the application thoroughly to ensure it performs as expected. Iterate on the design and functionality based on feedback and testing results.\n",
      "\n",
      "8. **Deployment**: Once satisfied with the application's performance, deploy it to your desired platform or environment.\n",
      "\n",
      "9. **Monitoring and Maintenance**: Continuously monitor the app's performance and make necessary updates or improvements over time.\n",
      "\n",
      "By following these steps and leveraging tools like Langchain and Pinecone, you can effectively build and deploy an application using LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{input}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.2)\n",
    "\n",
    "# Stuff documents chain\n",
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Full RAG chain\n",
    "rag_chain = create_retrieval_chain(retriever=hybrid_retriever, combine_docs_chain=document_chain)\n",
    "\n",
    "query = {\"input\": \"How can I build an app using LLMs\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"Q. {query['input']}\")\n",
    "print(f\"A. {response['answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1064d70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c400b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fb0ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac8fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
