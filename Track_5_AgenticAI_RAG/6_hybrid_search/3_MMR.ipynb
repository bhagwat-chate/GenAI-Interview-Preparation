{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2fc2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6686ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextLoader('mmr_rag_dataset.txt')\n",
    "raw_docs = data.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a09eb01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002060E142850>, search_type='mmr', search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type='mmr', search_kwargs={'k': 3})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5badbc22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002060E1492D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002060E149FD0>, root_client=<openai.OpenAI object at 0x000002060E148D50>, root_async_client=<openai.AsyncOpenAI object at 0x000002060E149D90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context provided.\n",
    "                                      \n",
    "                                      Context:\n",
    "                                      {context}\n",
    "\n",
    "                                      Question:\n",
    "                                      {input}\n",
    "\n",
    "                                      \n",
    "                                        \"\"\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35230e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaf3a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8052781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does LangChain supports agent and memory?',\n",
       " 'context': [Document(id='8cbefd99-d2e6-432c-94a4-9a365e915506', metadata={'source': 'mmr_rag_dataset.txt'}, page_content='Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\\nAgents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       "  Document(id='9cbc576a-6fa1-488a-9e39-ea2e8fd00ab0', metadata={'source': 'mmr_rag_dataset.txt'}, page_content='LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\\nLangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.'),\n",
       "  Document(id='254f18dc-df61-44cb-9256-7a278367e428', metadata={'source': 'mmr_rag_dataset.txt'}, page_content='BM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.\\nFAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.')],\n",
       " 'answer': 'LangChain supports agents by allowing LLMs to act as decision-makers that determine which tools to use and in what sequence during a task. These agents can utilize tools such as calculators, search APIs, or custom functions based on their instructions.\\n\\nWhen it comes to memory, LangChain enhances multi-turn conversation coherence with two types of memory: conversational memory and summarization memory. Conversational memory is managed using ConversationBufferMemory, while summarization memory is handled with ConversationSummaryMemory. This helps the models retain information from previous interactions, making future conversations more coherent and contextually aware.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\"input\": \"How does LangChain supports agent and memory?\"}\n",
    "\n",
    "response = rag_chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "900708b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain supports agents by allowing LLMs to act as decision-makers that determine which tools to use and in what sequence during a task. These agents can utilize tools such as calculators, search APIs, or custom functions based on their instructions.\n",
      "\n",
      "When it comes to memory, LangChain enhances multi-turn conversation coherence with two types of memory: conversational memory and summarization memory. Conversational memory is managed using ConversationBufferMemory, while summarization memory is handled with ConversationSummaryMemory. This helps the models retain information from previous interactions, making future conversations more coherent and contextually aware.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff34a241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doc: 1\n",
      "Memory in LangChain helps models retain previous interactions, making multi-turn conversations more coherent.\n",
      "Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\n",
      "\n",
      "Doc: 2\n",
      "LangChain allows LLMs to act as agents that decide which tool to call and in what order during a task.\n",
      "LangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.\n",
      "\n",
      "Doc: 3\n",
      "BM25 and vector-based retrieval can be combined in LangChain to support hybrid retrieval strategies.\n",
      "FAISS is a high-performance library for similarity search that LangChain leverages for efficient retrieval in RAG pipelines.\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(response):\n",
    "    print(f\"\\nDoc: {i+1}\")\n",
    "    print(response['context'][i].page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72616edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
