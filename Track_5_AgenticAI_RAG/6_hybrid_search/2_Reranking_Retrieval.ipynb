{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7759c8",
   "metadata": {},
   "source": [
    "### 2) Re-Ranking Technique\n",
    "\n",
    "**Re-ranking** is a second-stage filtering process in retrieval systems, especially in RAG pipelines, where we:\n",
    "\n",
    "1. First use a fast retriever (like BM25, FAISS, hybrid) to fetch top-k documents quickly.**\n",
    "\n",
    "2. Then use a more accurate but slower model (like a cross-encoder or LLM) to re-score and reorder those documents by relevance to the query.**\n",
    "\n",
    "ðŸ‘‰ It ensures that the most relevant documents appear at the top, improving the final answer from the LLM.\n",
    "\n",
    "### STEPS\n",
    "Step 1: Breadth (recall).\n",
    "\n",
    "Step 2: Coverage (lexical + semantic).\n",
    "\n",
    "Step 3: Focus (precision).\n",
    "\n",
    "Step 4: Reasoning (high-quality answer).\n",
    "\n",
    "This architecture balances speed + cost + accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83d33a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='369c50f5-a3d7-4e08-ad7d-e80b626c6957', metadata={'source': 'langchain_sample.txt'}, page_content='Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='48f54e7a-cd0d-4508-b284-b0d84a0fa561', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='07d0cd88-6097-4a50-92c5-19ef0cc4a989', metadata={'source': 'langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='912b2152-4a05-49a1-8852-27b16a4c0617', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.'),\n",
       " Document(id='4d99bebb-dbfa-45a4-a2c6-7bb884abe56e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='a25ea9dd-ac65-4a9c-95df-40bba3d01736', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='fd6bbeae-f221-4d47-a177-3abc2f2853e3', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.'),\n",
       " Document(id='99b57392-d3cc-486f-8ecc-43052e7766fe', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.'),\n",
       " Document(id='4a8f1acf-ce78-4a15-b154-520a8f4ec4e7', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.'),\n",
       " Document(id='c7403aaf-500a-4117-a9c2-4e7b07e6da27', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "embedding = OpenAIEmbeddings()\n",
    "loader = TextLoader(\"langchain_sample.txt\")\n",
    "data = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "docs = splitter.split_documents(data)\n",
    "query = \"How can I use langchain to build an application with memory and tools?\"\n",
    "\n",
    "# we need some vector store / DB so that we can implement the retrieveing. Here we can implement any fast search method like BM25, BOW, IF-IDF etcs\n",
    "\n",
    "vector_store = FAISS.from_documents(docs, embedding)\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# here reranking starts\n",
    "# Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Your task is to rank the following documents from most to least relevant.\n",
    "\n",
    "User Question: \"{question}\"\n",
    "\n",
    "Documents:\n",
    "{documents}\n",
    "\n",
    "Instructions:\n",
    "- Think about the relevance of each document to the user's question.\n",
    "- Return a list of document indices in ranked order, starting from the most relevant.\n",
    "\n",
    "Output format: comma-separated document indices (e.g., 2,1,3,0,...)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8abe1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)\n",
    "response = chain.invoke({\"question\": query, \"documents\": formatted_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4ed1a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The user is specifically asking about building an application with memory and tools using LangChain. Hereâ€™s a ranked list of documents based on their relevance to the user\\'s question:\\n\\n2, 1, 4, 3, 5, 7, 6, 8, 9, 10\\n\\nExplanation:\\n- Document 2 directly explains what LangChain is, including its support for \"memory\" and \"tools,\" which the user is interested in.\\n- Document 1 provides specific information about the memory component in LangChain, making it very relevant.\\n- Document 4 discusses the support for tool integration within LangChain, directly addressing the user\\'s interest in tools.\\n- Document 3 touches on agents, which are relevant as they involve tool use and decision-making, potentially incorporating memory.\\n- Document 5, although not directly related to memory or tools, mentions integrations that could aid in building applications, indirectly supporting the user\\'s task.\\n- Document 7 discusses RAG, which is somewhat related to using tools and external knowledge, but not directly about memory or the tool aspect the user asked for.\\n- Document 6, 8, 9, and 10 are more focused on retrieval methods and libraries, which are less directly applicable to memory and tool utilization within LangChain as per the userâ€™s specific query.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8933927",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lines = [f\"{i+1}. {doc.page_content}\" for i, doc in enumerate(retrieved_docs)]\n",
    "formatted_docs = \"\\n\".join(doc_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2f92745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.',\n",
       " '2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.',\n",
       " '3. Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.',\n",
       " '4. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.',\n",
       " '5. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.',\n",
       " '6. LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.',\n",
       " '7. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.',\n",
       " '8. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.',\n",
       " '9. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.',\n",
       " '10. BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f671124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\\n2. LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\\n3. Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\\n4. LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\\n5. LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\\n6. LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\\n7. Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\\n8. Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\\n9. FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\\n10. BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50e5659b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2, 1, 4, 3, 7, 5, 6, 9, 8, 10'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=chain.invoke({\"question\":query,\"documents\":formatted_docs})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e0fdf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 3, 2, 6, 4, 5, 8, 7, 9]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Parse and rerank\n",
    "indices = [int(x.strip()) - 1 for x in response.split(\",\") if x.strip().isdigit()]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46eade00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='369c50f5-a3d7-4e08-ad7d-e80b626c6957', metadata={'source': 'langchain_sample.txt'}, page_content='Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='48f54e7a-cd0d-4508-b284-b0d84a0fa561', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='07d0cd88-6097-4a50-92c5-19ef0cc4a989', metadata={'source': 'langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='912b2152-4a05-49a1-8852-27b16a4c0617', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.'),\n",
       " Document(id='4d99bebb-dbfa-45a4-a2c6-7bb884abe56e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='a25ea9dd-ac65-4a9c-95df-40bba3d01736', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='fd6bbeae-f221-4d47-a177-3abc2f2853e3', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.'),\n",
       " Document(id='99b57392-d3cc-486f-8ecc-43052e7766fe', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.'),\n",
       " Document(id='4a8f1acf-ce78-4a15-b154-520a8f4ec4e7', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.'),\n",
       " Document(id='c7403aaf-500a-4117-a9c2-4e7b07e6da27', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "909326fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='48f54e7a-cd0d-4508-b284-b0d84a0fa561', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.'),\n",
       " Document(id='369c50f5-a3d7-4e08-ad7d-e80b626c6957', metadata={'source': 'langchain_sample.txt'}, page_content='Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.'),\n",
       " Document(id='912b2152-4a05-49a1-8852-27b16a4c0617', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.'),\n",
       " Document(id='07d0cd88-6097-4a50-92c5-19ef0cc4a989', metadata={'source': 'langchain_sample.txt'}, page_content='Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.'),\n",
       " Document(id='fd6bbeae-f221-4d47-a177-3abc2f2853e3', metadata={'source': 'langchain_sample.txt'}, page_content='Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.'),\n",
       " Document(id='4d99bebb-dbfa-45a4-a2c6-7bb884abe56e', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.'),\n",
       " Document(id='a25ea9dd-ac65-4a9c-95df-40bba3d01736', metadata={'source': 'langchain_sample.txt'}, page_content='LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.'),\n",
       " Document(id='4a8f1acf-ce78-4a15-b154-520a8f4ec4e7', metadata={'source': 'langchain_sample.txt'}, page_content='FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.'),\n",
       " Document(id='99b57392-d3cc-486f-8ecc-43052e7766fe', metadata={'source': 'langchain_sample.txt'}, page_content='Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.'),\n",
       " Document(id='c7403aaf-500a-4117-a9c2-4e7b07e6da27', metadata={'source': 'langchain_sample.txt'}, page_content='BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_docs = [retrieved_docs[i] for i in indices if 0 <= i < len(retrieved_docs)]\n",
    "reranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "086df8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Final Reranked Results:\n",
      "\n",
      "Rank 1:\n",
      "LangChain is a flexible framework designed for developing applications powered by large language models (LLMs). It provides tools and abstractions to work with LLMs more effectively and includes components for prompt management, chains, memory, and agents.\n",
      "\n",
      "Rank 2:\n",
      "Memory in LangChain enables context retention across multiple steps in a conversation or task, making the application more coherent and stateful.\n",
      "\n",
      "Rank 3:\n",
      "LangChain supports tool integration including web search, calculators, and APIs, allowing LLMs to interact with external systems and respond more accurately to dynamic queries.\n",
      "\n",
      "Rank 4:\n",
      "Agents in LangChain are chains that use LLMs to decide which tools to use and in what order. This makes them suitable for multi-step tasks like question answering with search and code execution.\n",
      "\n",
      "Rank 5:\n",
      "Retrieval-Augmented Generation (RAG) is a powerful technique where external knowledge is retrieved and passed into the prompt to ground LLM responses. LangChain makes it easy to implement RAG using vector databases like FAISS, Chroma, and Pinecone.\n",
      "\n",
      "Rank 6:\n",
      "LangChain integrates with many third-party services such as OpenAI, Hugging Face, and Cohere. This enables developers to experiment with different models and optimize performance for specific use cases like summarization, question answering, or translation.\n",
      "\n",
      "Rank 7:\n",
      "LangChain supports hybrid retrieval by combining BM25 and dense similarity scores. This approach improves both precision and recall in document search.\n",
      "\n",
      "Rank 8:\n",
      "FAISS is a popular library used for fast approximate nearest neighbor search in high-dimensional spaces. It supports both flat and compressed indexes, which makes it scalable for large document stores.\n",
      "\n",
      "Rank 9:\n",
      "Dense retrieval uses embeddings to match query and documents in a vector space. This allows capturing semantic meaning, making it useful for fuzzy or natural language queries.\n",
      "\n",
      "Rank 10:\n",
      "BM25 is a traditional sparse retrieval method that scores documents based on keyword matching. Although fast, it often struggles with synonyms and semantic similarity.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Show results\n",
    "print(\"\\nðŸ“Š Final Reranked Results:\")\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\nRank {i}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908064e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b6b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0a1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
