{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190beced",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "235c4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docx import Document as Doc\n",
    "from langchain_community.document_loaders import Docx2txtLoader, UnstructuredWordDocumentLoader\n",
    "\n",
    "file_path=r\"D:\\Learning_Aug_2025\\final_gen_ai\\AgenticAI\\1_data_ingestion\\data\\doc\\RAGDataIngestion.docx\"\n",
    "\n",
    "doc_loader = Docx2txtLoader(file_path)\n",
    "\n",
    "doc_data = doc_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6f567e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len: 1\n",
      "page_content: üîπ RAG Interview Q&A ‚Äì Data Ingestion (LangChain Focus)\n",
      "\n",
      "\n",
      "\n",
      "Q1.\n",
      "\n",
      "As a GenAI Architect, how do you define ‚Äúdata ingestion‚Äù in a RAG system?\n",
      "\n",
      "Answer:\n",
      "Data ingestion is the first stage of a RAG pipeline, where raw enterprise data (PDFs, DOCX, HTML, CSV, emails, APIs, databases, etc.) is collected, normalized, and transformed into structured, retrievable units (documents/chunks).\n",
      "In LangChain, ingestion prepares unstructured data into a retrieval-ready format: document loaders ‚Üí chunking ‚Üí embedding ‚Üí indexing.\n",
      "Without a robust ingestion layer, retrieval accuracy, latency, and compliance downstream are compromised.\n",
      "\n",
      "\n",
      "\n",
      "Q2.\n",
      "\n",
      "What components of LangChain are typically used for ingestion?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Document Loaders ‚Üí Import from multiple sources (PDF, S3, Notion, Confluence, SQL, web pages).\n",
      "\n",
      "Text Splitters ‚Üí Break down large documents into semantically manageable chunks.\n",
      "\n",
      "RecursiveCharacterTextSplitter (preferred for preserving hierarchy).\n",
      "\n",
      "TokenTextSplitter (useful when token limits matter).\n",
      "\n",
      "Embeddings ‚Üí Convert chunks into dense vector representations.\n",
      "\n",
      "VectorStores ‚Üí Persist embeddings + metadata for retrieval.\n",
      "\n",
      "FAISS, Pinecone, Weaviate, Chroma, or AWS Kendra/OpenSearch.\n",
      "\n",
      "\n",
      "\n",
      "Q3.\n",
      "\n",
      "Walk me through a production-ready ingestion pipeline using LangChain.\n",
      "\n",
      "Answer:\n",
      "\n",
      "Load raw docs (multi-format, multi-source):\n",
      "\n",
      "\tfrom langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
      "\n",
      "\tloader = DirectoryLoader(\"s3://contracts/\", glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
      "\n",
      "\tdocuments = loader.load()\n",
      "\n",
      "Normalize & preprocess (clean text, remove boilerplate, standardize encoding).\n",
      "\n",
      "Chunk docs (preserve semantics & overlaps):\n",
      "\n",
      "\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "\n",
      "\tsplitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
      "\n",
      "\tchunks = splitter.split_documents(documents)\n",
      "\n",
      "Embed (domain-tuned model if available):\n",
      "\n",
      "\tfrom langchain_openai import OpenAIEmbeddings\n",
      "\n",
      "\tembeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
      "\n",
      "Index in Vector DB (with metadata):\n",
      "\n",
      "\tfrom langchain.vectorstores import FAISS\n",
      "\n",
      "\tdb = FAISS.from_documents(chunks, embeddings)\n",
      "\n",
      "\tdb.save_local(\"faiss_contracts_index\")\n",
      "\n",
      "Automate ingestion (scheduled ECS tasks / Airflow DAGs / CI/CD triggered).\n",
      "\n",
      "\n",
      "\n",
      "Q4.\n",
      "\n",
      "Why is chunking critical before embeddings?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Token limits: Embedding models cannot handle full books/docs.\n",
      "\n",
      "Semantic coherence: Smaller chunks ‚Üí embeddings capture a single idea/topic.\n",
      "\n",
      "Context stitching: Overlap ensures continuity across chunk boundaries.\n",
      "\n",
      "Retrieval precision: Smaller, focused vectors improve similarity search.\n",
      "\n",
      "As an architect, I balance chunk size vs retrieval latency: too small ‚Üí many DB hits; too large ‚Üí diluted embeddings.\n",
      "\n",
      "\n",
      "\n",
      "Q5.\n",
      "\n",
      "What metadata strategies would you use during ingestion?\n",
      "\n",
      "Answer:\n",
      "Metadata is the architect‚Äôs secret weapon for precision retrieval and compliance:\n",
      "\n",
      "Document ID, filename, source system.\n",
      "\n",
      "Page number / section headers.\n",
      "\n",
      "Timestamps (ingestion date, last updated).\n",
      "\n",
      "Access control labels (PII, compliance level).\n",
      "\n",
      "This enables filtered retrieval (e.g., ‚Äúlegal docs only,‚Äù ‚Äúlatest version only‚Äù) and supports auditability.\n",
      "\n",
      "\n",
      "\n",
      "Q6.\n",
      "\n",
      "What ingestion challenges do you anticipate in enterprise RAG, and how do you solve them?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Heterogeneous sources ‚Üí Solve with modular LangChain loaders.\n",
      "\n",
      "Scaling ingestion ‚Üí Use async pipelines, event-driven (AWS SQS/Lambda), or batch jobs (Airflow, ECS).\n",
      "\n",
      "Data quality ‚Üí Add OCR cleaning, deduplication, boilerplate removal.\n",
      "\n",
      "Re-ingestion (updates, deletes) ‚Üí Architect idempotent pipelines, versioning, delta updates.\n",
      "\n",
      "Security/compliance ‚Üí Encrypt data at rest (KMS), role-based access on vector DB.\n",
      "\n",
      "\n",
      "\n",
      "Q7. If you were ingesting data for LexiFlow (document intelligence system), how would you architect it?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Ingestion Sources: S3 (uploads), enterprise SharePoint, contract databases.\n",
      "\n",
      "Processing:\n",
      "\n",
      "Loaders: PyPDFLoader, CSVLoader, HTMLLoader.\n",
      "\n",
      "Splitter: Recursive splitter, tuned at 400 tokens + 50 overlap.\n",
      "\n",
      "Embedding: OpenAI text-embedding-3-large (fallback: HuggingFace Instructor-XL for offline).\n",
      "\n",
      "Storage: FAISS on EFS for PoC; Pinecone for production.\n",
      "\n",
      "Automation: GitHub Actions ‚Üí ECS task ‚Üí CloudWatch logs.\n",
      "\n",
      "Metadata tagging: doc_type (contract/report), source (S3/SharePoint), version, ingestion timestamp.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "üîπ RAG Interview Q&A ‚Äî Data Cleaning in Data Ingestion (GenAI Architect Level)\n",
      "\n",
      "\n",
      "\n",
      "Q1.\n",
      "\n",
      "Why is data cleaning an essential step in RAG data ingestion?\n",
      "\n",
      "Answer:\n",
      "Because embeddings reflect exactly what you feed them. If the source text contains noise (headers, disclaimers, OCR errors, duplicates), your retriever will surface irrelevant or redundant chunks ‚Üí leading to hallucinations and loss of user trust.\n",
      "For GenAI products, cleaning ensures accuracy, reliability, and compliance, which directly affect user adoption and enterprise ROI.\n",
      "\n",
      "\n",
      "\n",
      "Q2.\n",
      "\n",
      "What kinds of data quality issues do you typically encounter in enterprise RAG pipelines?\n",
      "\n",
      "Answer:\n",
      "\n",
      "OCR artifacts from scanned PDFs (‚Äú$‚Äù ‚Üí ‚ÄúS‚Äù, broken ligatures).\n",
      "\n",
      "Boilerplate noise (headers, watermarks, ‚ÄúPage X of Y‚Äù).\n",
      "\n",
      "Redundant copies of the same doc/version.\n",
      "\n",
      "Encoding mismatches (UTF-8 vs Latin-1).\n",
      "\n",
      "Empty/minimal chunks (‚ÄúTable continued on next page‚Äù).\n",
      "\n",
      "Sensitive data leakage (PII, PHI, PCI) that must be masked before embeddings.\n",
      "\n",
      "\n",
      "\n",
      "Q3.\n",
      "\n",
      "What strategies or techniques would you apply for cleaning in a RAG ingestion pipeline?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Normalization ‚Üí consistent casing, whitespace cleanup, punctuation standardization.\n",
      "\n",
      "Deduplication ‚Üí hash-based duplicate detection across docs/chunks.\n",
      "\n",
      "Regex filters ‚Üí remove page numbers, disclaimers, watermarks.\n",
      "\n",
      "Content filtering ‚Üí discard chunks below a minimum semantic length.\n",
      "\n",
      "NER-based redaction ‚Üí mask names, SSNs, addresses before embedding.\n",
      "\n",
      "Language filtering ‚Üí drop irrelevant languages or unsupported scripts.\n",
      "\n",
      "\n",
      "\n",
      "Q4.\n",
      "\n",
      "How would you integrate data cleaning with LangChain ingestion components?\n",
      "\n",
      "Answer:\n",
      "LangChain provides loaders and splitters, but cleaning must often be layered in as a preprocessing step. For example:\n",
      "\n",
      "for doc in documents:\n",
      "\n",
      "    text = doc.page_content\n",
      "\n",
      "    text = re.sub(r\"Page \\d+ of \\d+\", \"\", text)  # strip page numbers\n",
      "\n",
      "    text = text.strip().lower()  # normalize\n",
      "\n",
      "    doc.page_content = text\n",
      "\n",
      "I‚Äôd design this as a middleware layer between DocumentLoader and TextSplitter.\n",
      "For complex cases, integrate external libraries (BeautifulSoup, SpaCy, Presidio) with LangChain.\n",
      "\n",
      "\n",
      "\n",
      "Q5.\n",
      "\n",
      "How do you balance ‚Äúcleaning aggressively‚Äù vs ‚Äúretaining critical context‚Äù?\n",
      "\n",
      "Answer:\n",
      "This is an architectural trade-off:\n",
      "\n",
      "Over-cleaning ‚Üí risk of deleting legally/commercially critical information.\n",
      "\n",
      "Under-cleaning ‚Üí embeddings polluted with noise.\n",
      "Best practice:\n",
      "\n",
      "Configurable cleaning rules (stored in YAML/JSON, version-controlled).\n",
      "\n",
      "Domain-specific tuning (legal docs vs medical notes).\n",
      "\n",
      "Human-in-the-loop validation during pipeline rollout.\n",
      "\n",
      "\n",
      "\n",
      "Q6.\n",
      "\n",
      "How do you handle compliance and privacy during data cleaning?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Apply redaction pipelines (regex + NER) to anonymize PII/PHI.\n",
      "\n",
      "Maintain two versions: raw (restricted access) and cleaned (used for embeddings).\n",
      "\n",
      "Ensure audit logs capture what was removed/redacted.\n",
      "\n",
      "Encrypt data at rest (KMS) and enforce role-based access to raw vs cleaned layers.\n",
      "\n",
      "\n",
      "\n",
      "Q7.\n",
      "\n",
      "What metrics would you track to validate the effectiveness of cleaning?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Noise reduction ratio (percentage of boilerplate removed).\n",
      "\n",
      "Chunk discard rate (too short/empty after cleaning).\n",
      "\n",
      "Deduplication savings (reduced doc count).\n",
      "\n",
      "PII detection coverage (entities masked per 1k tokens).\n",
      "\n",
      "Impact on retrieval precision (measured via evaluation datasets).\n",
      "\n",
      "\n",
      "\n",
      "Q8.\n",
      "\n",
      "If you were designing LexiFlow‚Äôs ingestion pipeline, how would you build the cleaning stage?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Stage 1: Raw ingestion from S3, SharePoint, CSVs, PDFs.\n",
      "\n",
      "Stage 2: Cleaning service:\n",
      "\n",
      "Boilerplate & duplicate removal.\n",
      "\n",
      "Regex for headers/footers.\n",
      "\n",
      "PII masking (Aadhaar, PAN, phone numbers).\n",
      "\n",
      "Stage 3: Validation hooks: log % text removed; flag abnormal removals.\n",
      "\n",
      "Stage 4: Hand off to chunking + embeddings.\n",
      "I‚Äôd containerize this cleaning service as a microservice in ECS Fargate, so it scales independently and is reusable across products.\n",
      "\n",
      "\n",
      "\n",
      "Q9.\n",
      "\n",
      "How would you future-proof data cleaning pipelines as enterprise content grows?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Make cleaning modular and rule-driven ‚Üí new domains just add configs.\n",
      "\n",
      "Integrate ML-based classifiers to auto-detect noise patterns at scale.\n",
      "\n",
      "Automate continuous evaluation ‚Üí compare ‚Äúpre-cleaned vs post-cleaned retrieval precision‚Äù.\n",
      "\n",
      "Treat cleaning configs as code (in Git) ‚Üí enabling auditability, rollbacks, and CI/CD for ingestion.\n",
      "metadata: {'source': 'D:\\\\Learning_Aug_2025\\\\final_gen_ai\\\\AgenticAI\\\\1_data_ingestion\\\\data\\\\doc\\\\RAGDataIngestion.docx'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"len: {len(doc_data)}\")\n",
    "print(f\"page_content: {doc_data[0].page_content}\")\n",
    "print(f\"metadata: {doc_data[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a239a0c",
   "metadata": {},
   "source": [
    "### Method 2: Unstructured Word Doc Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03174cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_loader_unstruct = UnstructuredWordDocumentLoader(file_path, mode='elements')\n",
    "doc_data = doc_loader_unstruct.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7639f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk count: 146\n",
      "page_content: üîπ RAG Interview Q&A ‚Äì Data Ingestion (LangChain Focus)\n",
      "metadata: {'source': 'D:\\\\Learning_Aug_2025\\\\final_gen_ai\\\\AgenticAI\\\\1_data_ingestion\\\\data\\\\doc\\\\RAGDataIngestion.docx', 'category_depth': 0, 'emphasized_text_contents': ['üîπ', 'RAG Interview Q&A ‚Äì Data Ingestion (LangChain Focus)'], 'emphasized_text_tags': ['b', 'b'], 'file_directory': 'D:\\\\Learning_Aug_2025\\\\final_gen_ai\\\\AgenticAI\\\\1_data_ingestion\\\\data\\\\doc', 'filename': 'RAGDataIngestion.docx', 'last_modified': '2025-09-17T12:30:49', 'page_number': 1, 'languages': ['eng'], 'filetype': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document', 'category': 'UncategorizedText', 'element_id': '077f0cbfd56091095b598cc607ce42c5'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"chunk count: {len(doc_data)}\")\n",
    "print(f\"page_content: {doc_data[0].page_content}\")\n",
    "print(f\"metadata: {doc_data[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c33eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in enumerate(doc_data):\n",
    "    print(v.page_content)\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c5f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c2256f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b7c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a7a607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
