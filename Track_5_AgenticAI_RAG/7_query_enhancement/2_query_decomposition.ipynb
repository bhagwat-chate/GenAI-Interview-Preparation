{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffeb756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da36dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning_Aug_2025\\final_gen_ai\\AgenticAI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7c7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader('langchain_crewai_dataset.txt')\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "llm = ChatOpenAI(model='o4-mini')\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vector_store.as_retriever(search_type='mmr', search_kwargs={'k':4, \"lambda_mult\": 0.7})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53eec7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd47bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001EF04C0C790>, search_type='mmr', search_kwargs={'k': 4, 'lambda_mult': 0.7})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "279e96d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\nYou are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\\n\\nQuestion:\\n\"{question}\"\\n\\nSub-questions:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001EF7FC40B50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001EF01203B50>, root_client=<openai.OpenAI object at 0x000001EF7FC42850>, root_async_client=<openai.AsyncOpenAI object at 0x000001EF012034D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "Question:\n",
    "\"{question}\"\n",
    "\n",
    "Sub-questions:\n",
    "\"\"\")\n",
    "\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()\n",
    "decomposition_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67dd095b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. What memory architectures and storage mechanisms does LangChain use (e.g. conversational, summary, or vector-store memory)?  \n",
      "2. How does CrewAI implement and manage memory in its framework?  \n",
      "3. What kinds of agents (tool-using, planner, router, etc.) does LangChain provide and how do they orchestrate tasks?  \n",
      "4. How do CrewAI’s agent designs and orchestration patterns differ from those in LangChain?\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question = decomposition_chain.invoke({\"question\": query})\n",
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "310cb871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nUse the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{input}\\n\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001EF7FC40B50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001EF01203B50>, root_client=<openai.OpenAI object at 0x000001EF7FC42850>, root_async_client=<openai.AsyncOpenAI object at 0x000001EF012034D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "                                         \n",
    "Context:\n",
    "{context}\n",
    "                                         \n",
    "Question:\n",
    "{input}\n",
    "                                         \n",
    "\"\"\")\n",
    "\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc4b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "\n",
    "    sub_qs_txt = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_question = [q.strip(\"-1234567890. \").strip() for q in sub_qs_txt.split(\"\\n\") if q.strip()]\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for subq in sub_question:\n",
    "        docs = retriever.invoke(subq)\n",
    "        response = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        result.append(f\"Q: {subq}\\nA: {response}\")\n",
    "\n",
    "    return\"\\n\\n\".join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df338989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Here are four focused sub-questions to guide your document retrieval:\n",
      "A: Below are four targeted sub‐questions you can use to pull back the most relevant documents or knowledge snippets from your store:\n",
      "\n",
      "1. How does “knowledge fetching and injection” work in version 4—what mechanisms are used to select, filter, and embed external knowledge into the LLM prompt, and what impact does that have on answer accuracy and hallucination rates?  \n",
      "2. What architecture and algorithms power LangChain’s hybrid retrieval in v4—specifically, how are sparse (e.g. BM25) and dense (embedding) methods combined, and what recall/precision trade-offs or performance gains have been observed?  \n",
      "3. In what ways does CrewAI orchestrate specialized agents for multi-step workflows (e.g. market research, legal analysis, product dev, coding), and what coordination, hand-off, or chaining patterns does it employ to boost overall throughput and quality?  \n",
      "4. Which external tools (web search, calculators, code execution sandboxes, custom APIs) does the system support, and how are they invoked, composed, and secured within a single end-to-end LangChain/CrewAI pipeline?\n",
      "\n",
      "Q: What memory features (e.g. short-term vs. long-term, vector stores, retrieval strategies) does LangChain offer and how are they used in a typical workflow?\n",
      "A: LangChain’s “memory” abstraction comes in two flavors:  \n",
      "1) Short-term (conversational) memory – keeps you “in context” during a single session  \n",
      "2) Long-term (persistent) memory – lets you recall facts or documents across sessions  \n",
      "\n",
      "Below is a survey of the most common memory modules you’ll encounter, how they differ, and how they plug into a typical LLM workflow.  \n",
      "\n",
      "1. Short-Term (Session) Memory  \n",
      "   • ConversationBufferMemory  \n",
      "     – Simply appends every user/input ↔ LLM/output turn to an in-memory buffer.  \n",
      "     – On each new call you inject the entire buffer into your prompt so the model “remembers” the immediate back-and-forth.  \n",
      "     – Good for short chats or when you want an exact replay of recent turns.  \n",
      "   • ConversationBufferWindowMemory  \n",
      "     – Like BufferMemory but only keeps the last N turns to bound your token usage.  \n",
      "     – Older turns are dropped rather than summarized.  \n",
      "   • ConversationSummaryMemory  \n",
      "     – Keeps a running (or rolling) summary of everything beyond a certain window.  \n",
      "     – As the conversation grows, it re-summarizes older portions so you can stay under your prompt token limit while retaining “long-ago” context.  \n",
      "\n",
      "2. Long-Term (Persistent) Memory  \n",
      "   • Vector-store based memories (Pinecone, FAISS, Weaviate, Chroma, Redis, etc.)  \n",
      "     – You embed either raw user utterances or extracted “facts” from the conversation.  \n",
      "     – When you get a new query, you run a similarity-search retriever over that vector store (e.g. top-k most relevant facts) and prepend them to your prompt.  \n",
      "     – Enables true “episodic” recall across days or weeks of interaction.  \n",
      "   • SQL/NoSQL (Redis, PostgreSQL, MongoDB)  \n",
      "     – Store arbitrary key–value pairs (e.g. user preferences, profile data).  \n",
      "     – Retrieval is a simple key lookup, then the values get passed into your prompt.  \n",
      "\n",
      "3. Retrieval Strategies  \n",
      "   • Similarity Search: top-k embeddings match  \n",
      "   • Time-based: retrieve only the most recent M entries  \n",
      "   • Hybrid: filter by key/value metadata (e.g. tags or timestamps) then run similarity search  \n",
      "\n",
      "Typical Workflow  \n",
      "1) Build your components:  \n",
      "   – memory = ConversationBufferMemory(…) or VectorStoreRetrieverMemory(…)  \n",
      "   – prompt = PromptTemplate(template=“…{history}{input}…”)  \n",
      "   – llm_chain = LLMChain(llm=OpenAI(…), prompt=prompt, memory=memory)  \n",
      "\n",
      "2) On each user message:  \n",
      "   a. memory.load_memory_variables({ \"input\": user_message })  \n",
      "      • for buffer memory: returns the recent chat turns  \n",
      "      • for vector memory: runs a similarity search and returns top-k facts  \n",
      "   b. prompt.format(**memory_vars, input=user_message)  \n",
      "   c. llm.generate(formatted_prompt) → new_response  \n",
      "   d. memory.save_context({\"input\": user_message}, {\"output\": new_response})  \n",
      "      • buffer/summarizer will append or summarize into its in-memory store  \n",
      "      • vector memory will turn new_response (or extracted facts) into embeddings and upsert into the vector store  \n",
      "\n",
      "3) Return new_response to the user  \n",
      "\n",
      "By mixing and matching these modules you get:  \n",
      "– A lightning-fast turn-by-turn chat (buffer memory)  \n",
      "– Summarized history to respect token limits (summary memory)  \n",
      "– Persistent, RAG-style retrieval for evergreen user data or domain knowledge (vector store memory)\n",
      "\n",
      "Q: How does CrewAI implement and manage memory (context storage, retrieval policies, persistence) within its pipeline?\n",
      "A: CrewAI’s memory subsystem is deliberately pluggable and consists of three main parts: a short-term “context buffer,” a longer-term “memory store,” and a retrieval policy layer that governs what gets pulled back in at each step.  Under the hood it looks roughly like this:\n",
      "\n",
      "1. Agent Context Buffer (Ephemeral)  \n",
      "   • Every agent run keeps an in-memory, append-only log of its own inputs, outputs, observations and tool-use results for the current session or turn.  \n",
      "   • This buffer is what gets streamed straight into the prompt (either raw or via a rolling window), so you always get the last N messages (by token or turn count) for continuity.\n",
      "\n",
      "2. Memory Store (Persistent or Semi-persistent)  \n",
      "   • Exposes a simple key–value or document interface.  By default CrewAI will drop all session memory when the process ends, but you can swap in built-in adapters for JSON files, SQLite, Redis, Postgres or any vector database (Pinecone, Weaviate, etc.).  \n",
      "   • The store is updated either:  \n",
      "     – Automatically at the end of each agent turn (everything in the context buffer beyond a configurable age threshold gets “committed”).  \n",
      "     – Or manually via a “memory” tool call inside your agent’s prompt sequence.\n",
      "\n",
      "3. Retrieval Policy Layer  \n",
      "   • Before you invoke the LLM on a new turn, CrewAI runs a retrieval pass over the memory store.  You configure this with simple rules or “recipes,” for example:  \n",
      "     – Recency window (e.g. last 5000 tokens or last 10 turns)  \n",
      "     – Semantic relevance via vector‐embeddings (score threshold or top-k)  \n",
      "     – Tag or namespace filters (e.g. only pull memories tagged “customer­-profile”)  \n",
      "   • You can mix these policies.  For example: “return all memories from the last 24 hours plus the top 3 semantically relevant ones.”\n",
      "\n",
      "4. Summarization/Compression (Optional)  \n",
      "   • If your memory store grows large, CrewAI can fire off a summarization step on older memories (configured via age or size triggers).  \n",
      "   • Summaries replace raw logs so you keep the gist without blowing past your token‐limits.\n",
      "\n",
      "5. Persistence and Lifecycle Hooks  \n",
      "   • On startup you can load an existing memory dump so agents pick up where they left off.  \n",
      "   • On shutdown (or on a cron schedule) the system can auto-flush its in-memory buffer into your chosen backend.  \n",
      "   • There are simple JSON-export and –import commands for snapshotting or migrating your store.\n",
      "\n",
      "Putting it all together, each turn looks like:  \n",
      "  a) Pull in the last N messages + any retrieved long-term memories per your policies  \n",
      "  b) Run the LLM (possibly in parallel with other agents)  \n",
      "  c) Append the new outputs & observations to the context buffer  \n",
      "  d) Commit eligible buffer entries into the persistent store (or hand that off to an async writer)  \n",
      "  e) Optionally trigger a summarization pass on the oldest entries  \n",
      "\n",
      "Because you define both the store (in-memory, file, SQL, vector DB) and the retrieval policy declaratively in your crew’s YAML/JSON config, you get full control over context‐storage, recall behavior, and persistence without touching the core loop.\n",
      "\n",
      "Q: What agent architecture and orchestration patterns (planners, tool-using agents, action loops) does LangChain provide?\n",
      "A: LangChain’s “agents” are really just a particular orchestration layer on top of its core LLM‐and‐chain abstractions.  At its heart is a two-part, planner–executor architecture plus a looping mechanism that lets you dynamically call out to any number of tools (APIs, vector stores, databases, whatever) until you arrive at a final answer.  \n",
      "\n",
      "1. Planner–Executor split  \n",
      "   • The Planner (often an LLM prompt template under the hood) “thinks” about the user’s goal and produces a *plan* – essentially a list of actions or tool calls (sometimes as step-by-step Chain-of-Thought, sometimes as a structured JSON plan, etc.).  \n",
      "   • The Executor takes that plan, invokes the requested tool(s) in order, captures their outputs (“observations”), and feeds those back into the context.  \n",
      "\n",
      "2. Action Loop  \n",
      "   • Thought → Action → Observation → (append to context) → Thought …  \n",
      "   • This loop runs until the Planner decides it has enough to return a final answer rather than another tool call.  \n",
      "\n",
      "3. Built-in Planner/Agent Flavors  \n",
      "   • Zero-Shot Agent – picks tools on the fly purely from an LLM prompt + tool descriptions  \n",
      "   • Few-Shot / ReAct Agent – uses in-prompt examples to teach the LLM when/how to call tools in a “Reason → Act” style  \n",
      "   • Conversational Agent – same idea but with chat history and stateful memory baked in  \n",
      "   • MRKL-style planners – break down problems into sub-tasks via a more structured intermediate representation  \n",
      "\n",
      "4. Orchestration Patterns (Chains + Routers)  \n",
      "   • SequentialChain – a fixed linear pipeline of calls  \n",
      "   • RouterChain (aka “multi-chain”) – inspect the input and route to one of several sub-chains or agents  \n",
      "   • MultiActionChain – allow multiple tool calls in a single execution step, returning an array of results  \n",
      "   • Custom planners/executors – drop in your own logic or LLM prompts for planning, or swap out the executor to run tools in parallel, batch mode, async, etc.  \n",
      "\n",
      "5. Branching Logic & Memory  \n",
      "   • Because every tool-call observation gets appended back to the LLM’s context – and you can optionally swap in any of LangChain’s memory modules – you can build quite complex, stateful flows with dynamic branching based on intermediate results.  \n",
      "\n",
      "In practice you pick one of the out-of-the-box AgentExecutor classes (ZeroShotAgent, ConversationalAgent, ReActAgent, etc.), register whatever Tools you need, and then call `.run(input)` (or step through the `.plan` + `.execute` APIs yourself).  Behind the scenes you’re sitting in that planner/executor action-loop, calling tools until the planner says “I’m done,” at which point you get your final answer.\n",
      "\n",
      "Q: How are agents structured in CrewAI (decision-making process, tool integration, execution flow) and how do they differ from LangChain’s agents?\n",
      "A: In CrewAI, agents are not “one big LLM + tool picker” but rather a small team of role-specialized LLMs collaborating according to a predefined workflow.  Here’s roughly how they’re structured and how that differs from a typical LangChain agent:\n",
      "\n",
      "1. Decision-making process  \n",
      "   • Role specialization – tasks get split among distinct roles (for example: a “Manager” that breaks down the problem, one or more “Researchers” that gather and triage information, an “Executor” or “Implementer” that actually calls tools or writes code, and a “Reviewer” or “Summarizer” that checks results and produces the final answer).  \n",
      "   • Hierarchical or consensus-style orchestration – the Manager assigns subtasks, researchers report back, conflicts can be flagged for review, and the team either votes or the Manager adjudicates the final call.  \n",
      "\n",
      "2. Tool integration  \n",
      "   • Role-aware tool access – each agent has a smaller, purpose-built toolset instead of exposing every tool to a single monolithic agent. E.g. Researchers might get access to search or database tools, Executors get code runners or API clients, Reviewers get evaluation scripts.  \n",
      "   • Dynamic tool-role mapping – you can plug in new tools at the team level and bind them to the roles that need them. Tools aren’t hard-wired into the LLM’s action loop; they are injected into the right subagent’s context.  \n",
      "\n",
      "3. Execution flow  \n",
      "   • Task decomposition – user input → Manager breaks into steps → Researchers fetch data → Executor invokes tools or synthesizes code → Reviewer verifies or refines → final output.  \n",
      "   • Horizontal scaling – you can spin up multiple Researchers in parallel on different data sources, or multiple Executors on different subtasks.  \n",
      "   • Vertical depth – you can iterate: after the first pass the Reviewer can ask Researchers for more evidence, or ask the Executor to revise code, adding reasoning “layers” without blowing up a single chain-of-thought prompt.  \n",
      "\n",
      "How that differs from a LangChain agent  \n",
      "   • Monolithic vs. modular – LangChain agents typically combine “think → decide on tool → call tool → think → return” all in one loop. CrewAI breaks that loop into separate LLMs with distinct jobs.  \n",
      "   • Single-agent chain of thought vs. multi-agent collaboration – LangChain’s depth is limited by how far you can push chain-of-thought in one prompt. CrewAI gives you multiple agents that can debate, iterate, and review.  \n",
      "   • Static tool wrapper vs. dynamic role binding – in LangChain you register tools on the Agent or AgentExecutor level and the agent picks among them. In CrewAI you map tools to roles and only surface them to the right subagent.  \n",
      "   • Limited concurrency vs. elastic scaling – LangChain runs one agent at a time (or you manually orchestrate multiple chains). CrewAI has built-in support for horizontal scaling (spawn more Researchers or Executors) and vertical scaling (add deeper review loops) via its role orchestration.  \n",
      "\n",
      "In short, CrewAI transforms a single LLM-centric agent into a mini “crew” of specialized agents, each with its own decision logic, tool access, and hand-off points—whereas LangChain agents are more like a single LLM + chain-of-thought planner that picks and invokes tools in a single loop.\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "response = full_query_decomposition_rag_pipeline(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9b564",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
