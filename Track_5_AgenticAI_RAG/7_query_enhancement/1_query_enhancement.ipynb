{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc5d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95cadb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffc2e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chunks: 241\n",
      "first chunks: page_content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration. Developers can use' metadata={'source': 'query_enhancement_dataset.txt'}\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader(\"query_enhancement_dataset.txt\")\n",
    "raw_docs = data_loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n",
    "print(f'total chunks: {len(chunks)}')\n",
    "print(f'first chunks: {chunks[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7283f8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2bca6ce0890>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd6d464b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002BCA6CE0890>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_type='mmr', search_kwargs={'k':5})\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cfae3d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002BCE48353D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002BCA79025D0>, root_client=<openai.OpenAI object at 0x000002BCAA7D5710>, root_async_client=<openai.AsyncOpenAI object at 0x000002BCE4835490>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(model='o4-mini')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10a5c1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\n    You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant technical terms, and useful context.\\n\\n    Original query: \"{query}\"\\n\\n    Expanded query:\\n\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002BCE48353D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002BCA79025D0>, root_client=<openai.OpenAI object at 0x000002BCAA7D5710>, root_async_client=<openai.AsyncOpenAI object at 0x000002BCE4835490>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant technical terms, and useful context.\n",
    "\n",
    "    Original query: \"{query}\"\n",
    "\n",
    "    Expanded query:\n",
    "    \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_expansion_chain = prompt | llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "475d5aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:  \\n“LangChain memory modules, memory management and retention strategies for conversational AI—short-term vs. long-term memory, memory buffer, summary memory, conversational history storage; vector-store integration (Chroma, FAISS, Pinecone, Redis) for persistent memory; retrieval-augmented generation (RAG) with LangChain; Memory classes and API usage; memory middleware patterns; optimizing context windows; stateful agent workflows in LangChain documentation.”'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\"query\": \"LangChain memory\"}\n",
    "\n",
    "response = query_expansion_chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e24916a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adaf377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“LangChain memory modules conversation history management”  \n",
      "“LangChain ConversationBufferMemory”  \n",
      "“LangChain ConversationSummaryMemory”  \n",
      "“LangChain vector‐store memory RAG”  \n",
      "“persistent memory Redis FAISS Pinecone Chroma”  \n",
      "“LangChain memory classes API reference tutorial”  \n",
      "“LLM app state management best practices”\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "599a5910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n                                             Answer the question based on the context below.\\n\\n                                             Context:\\n                                             {context}\\n\\n                                             Question: {input}\\n                                             ')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "                                             Answer the question based on the context below.\n",
    "\n",
    "                                             Context:\n",
    "                                             {context}\n",
    "\n",
    "                                             Question: {input}\n",
    "                                             \"\"\")\n",
    "answer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58d9585a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n                                             Answer the question based on the context below.\\n\\n                                             Context:\\n                                             {context}\\n\\n                                             Question: {input}\\n                                             ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002BCE48353D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002BCA79025D0>, root_client=<openai.OpenAI object at 0x000002BCAA7D5710>, root_async_client=<openai.AsyncOpenAI object at 0x000002BCE4835490>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain = create_stuff_documents_chain(llm=llm, prompt=answer_prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f2e1b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  input: RunnableLambda(...),\n",
       "  context: RunnableLambda(...)\n",
       "}\n",
       "| RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableLambda(format_docs)\n",
       "  }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "  | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n                                             Answer the question based on the context below.\\n\\n                                             Context:\\n                                             {context}\\n\\n                                             Question: {input}\\n                                             ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002BCE48353D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002BCA79025D0>, root_client=<openai.OpenAI object at 0x000002BCAA7D5710>, root_async_client=<openai.AsyncOpenAI object at 0x000002BCE4835490>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x['input'],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x['input']}))\n",
    "    })\n",
    "    | document_chain\n",
    ")\n",
    "\n",
    "rag_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62918946",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = {\"query\": \"What type of memory does LangChain support?\"}\n",
    "query2 = {\"input\": \"What type of memory does LangChain support?\"}\n",
    "\n",
    "enhanced_query = query_expansion_chain.invoke(query1)\n",
    "response = rag_pipeline.invoke(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6fad23fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "\n",
      "“What memory modules and backends does LangChain provide? Specifically, what short-term versus long-term memory types are supported (e.g. in-memory buffers vs. summary memory), which built-in classes and interfaces exist (ConversationBufferMemory, ConversationSummaryMemory, CombinedMemory, ChatMessageHistory, etc.), and what external storage backends can be plugged in (RedisMemory, SQLMemory/SQLite/PostgreSQL, vector-store memory via Chroma/Pinecone/FAISS/Weaviate/Milvus)? How do memory_key, return_messages, k (for k-NN retrieval), and other memory parameters work in the LangChain Python SDK, and how is memory used in RAG pipelines, multi-turn chatbots, and agent applications?”\n"
     ]
    }
   ],
   "source": [
    "print(enhanced_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f5511",
   "metadata": {},
   "source": [
    "enhanced_query:\n",
    "“What memory modules and backends does LangChain provide? Specifically, what short-term versus long-term memory types are supported (e.g. in-memory buffers vs. summary memory), which built-in classes and interfaces exist (ConversationBufferMemory, ConversationSummaryMemory, CombinedMemory, ChatMessageHistory, etc.), and what external storage backends can be plugged in (RedisMemory, SQLMemory/SQLite/PostgreSQL, vector-store memory via Chroma/Pinecone/FAISS/Weaviate/Milvus)? How do memory_key, return_messages, k (for k-NN retrieval), and other memory parameters work in the LangChain Python SDK, and how is memory used in RAG pipelines, multi-turn chatbots, and agent applications?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc3562af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain provides conversational “memory” modules—most notably:  \n",
      "• ConversationBufferMemory – keeps a running buffer of past turns  \n",
      "• ConversationSummaryMemory – summarizes long or growing dialogues to stay within token limits\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd4224e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = {\"query\": \"What type of memory does CrewAI support?\"}\n",
    "query2 = {\"input\": \"What type of memory does CrewAI support?\"}\n",
    "\n",
    "enhanced_query = query_expansion_chain.invoke(query1)\n",
    "response = rag_pipeline.invoke(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1a3bbb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“What types of memory mechanisms and stores does the CrewAI conversational platform support—e.g. short-term (session) vs. long-term (persistent) memory, episodic or user-profile memory, knowledge-base memory, vector-embedding stores or semantic RAG memory, file/document memory, cache vs. database backends (e.g. Pinecone, Weaviate, Redis), context-window limits, memory indexing approaches (inverted index, FAISS), memory pruning/garbage-collection strategies, and how these interact with CrewAI’s LLM prompting pipeline?”\n"
     ]
    }
   ],
   "source": [
    "print(enhanced_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c2429c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not mention or specify any memory model or memory type that CrewAI supports.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
